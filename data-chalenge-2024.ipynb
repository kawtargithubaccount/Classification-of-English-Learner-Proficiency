{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70908,"databundleVersionId":7748349,"sourceType":"competition"}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import os\n#import re\nimport nltk\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom transformers import DistilBertTokenizer , AutoTokenizer\nfrom transformers import TFDistilBertForSequenceClassification  , AutoModelForTokenClassification ,AutoModelForSequenceClassification, BertConfig,BertForSequenceClassification\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_punct(text):\n\n#     new_text = word_tokenize(text)\n#     new_text = list(filter(lambda token: token not in string.punctuation, new_text))\n#     text = \" \".join([word for word in new_text])\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    new_text = tokenizer.tokenize(text)\n    text = \" \".join([word for word in new_text])\n\n    return text.strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Importer les bibliothèques nécessaires\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Préparer les données\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer):\n        self.tokenizer = tokenizer\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        inputs = self.tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n        item = {key: val.squeeze() for key, val in inputs.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Charger votre dataset\ndf = pd.read_csv('/kaggle/input/celp/train_data.csv') # Mettre à jour avec le chemin correct\ndf[\"text\"] = df[\"text\"].apply(lambda text :remove_punct(text))\ntexts = df['text'].tolist()\nlabels = df['level'].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modélisation","metadata":{}},{"cell_type":"code","source":"def one_hot_encoding(labels, num_classes):\n    return torch.eye(num_classes)[labels]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Diviser les données\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.15)\ntrain_labels = one_hot_encoding(train_labels,6)\nval_labels = one_hot_encoding(val_labels,6)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialiser le tokenizer\n#tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained('AbdulSami/bert-base-cased-cefr',device = device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Créer les datasets\ntrain_dataset = TextDataset(train_texts, train_labels, tokenizer)\nval_dataset = TextDataset(val_texts, val_labels, tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Charger DistilBERT pré-entraîné\n#model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6)#\n#model = AutoModelForTokenClassification.from_pretrained('AbdulSami/bert-base-cased-cefr', num_labels=6)\nmodel_name = 'AbdulSami/bert-base-cased-cefr'\nmodel_name = '/kaggle/working/results/checkpoint-2176'\n\nconfig = BertConfig.from_pretrained(model_name, num_labels=6)\n\nmodel = BertForSequenceClassification.from_pretrained(model_name, config=config)\n\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Définir le DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install evaluate\nimport evaluate\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    labels = np.argmax(labels,axis = 1)\n    return accuracy.compute(predictions=predictions, references=labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Entraîner le modèle\ntraining_args = TrainingArguments(\n    output_dir='./results',          \n    num_train_epochs=5,              \n    per_device_train_batch_size=16,  \n    per_device_eval_batch_size=16,   \n    weight_decay=0.02,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir='./logs',            \n)\n\nfrom sklearn.utils import compute_class_weight\nclass_weights = compute_class_weight(\n                                        class_weight = \"balanced\",\n                                        classes = np.unique(df['level']),\n    y = df['level']\n    \n)\ndef one_hot_encoding(labels, num_classes):\n    return torch.eye(num_classes)[labels]\n\nclass CustomTrainer(Trainer):\n    \n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\")\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # compute custom loss (suppose one has 6 labels with different weights)\n        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device))\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), one_hot_encoding(labels.to(\"cpu\"),self.model.config.num_labels).to(device))\n        \n        return (loss, outputs) if return_outputs else loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wandb login --relogin 5f8ae5d90931deb3c0da631b259c98acbb7b9bcb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nimport torch.nn as nn\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',          \n    num_train_epochs=10,              \n    per_device_train_batch_size=16,  \n    per_device_eval_batch_size=16,   \n    weight_decay=0.02,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir='./logs',            \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Évaluer le modèle\ntrainer.evaluate()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtenir les résultats de la prédiction\npredictions = trainer.predict(val_dataset)\n\n# Les logits des prédictions sont dans `predictions.predictions`\nlogits = predictions.predictions\n\n# Convertir les logits en indices de classe prédite\ny_pred = logits.argmax(axis=-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(val_dataset.labels, y_pred)\nfig, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3, 4, 5], yticklabels=[0, 1, 2, 3, 4, 5])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(val_dataset.labels, y_pred)\nfig, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3, 4, 5], yticklabels=[0, 1, 2, 3, 4, 5])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix with custom loss')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtenir les résultats de la prédiction\npredictions = trainer.predict(train_dataset)\n\n# Les logits des prédictions sont dans `predictions.predictions`\nlogits = predictions.predictions\n\n# Convertir les logits en indices de classe prédite\ny_pred_train = logits.argmax(axis=-1)\n\ncm = confusion_matrix(train_dataset.labels, y_pred_train)\n\n \nfig, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3, 4, 5], yticklabels=[0, 1, 2, 3, 4, 5])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import DataLoader\n\n# Charger les données de test\ntest_df = pd.read_csv(\"/kaggle/input/celp/test_data.csv\")  # Mettre à jour avec le chemin correct\ntest_df[\"text\"] = test_df[\"text\"].apply(lambda text :remove_punct(text))\ntest_texts = test_df['text'].tolist()\ntest_ids = test_df['Id'].tolist()\n\n# Préparer le dataset de test\nclass TestDataset(Dataset):\n    def __init__(self, texts, tokenizer):\n        self.tokenizer = tokenizer\n        self.texts = texts\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        inputs = self.tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n        item = {key: val.squeeze() for key, val in inputs.items()}\n        return item\n\ntest_dataset = TestDataset(test_texts, tokenizer)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Faire des prédictions\nmodel.eval()  # Mettre le modèle en mode évaluation\npredictions = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        outputs = model(**{k: v.to(model.device) for k, v in batch.items()})\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=-1)\n        predictions.extend(preds.cpu().numpy())\n\n# Préparer le fichier de soumission\nsubmission_df = pd.DataFrame({'Id': test_ids, 'level': predictions})\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"Fichier de soumission créé avec succès !\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Bert Finetuning using LORA Model ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# Créer les datasets\ntrain_dataset = TextDataset(train_texts, train_labels, tokenizer)\nval_dataset = TextDataset(val_texts, val_labels, tokenizer)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install peft\nfrom peft import LoraConfig, TaskType\nfrom peft import get_peft_model\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, r=1, lora_alpha=1, lora_dropout=0.1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-cased', \n    num_labels=6\n)\nmodel = get_peft_model(model, lora_config)\n\n# Regarder le nombre de paramètre d'entrainement :\ndef count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(\"The nom\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate\nimport numpy as np\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          \n    num_train_epochs=5,              \n    per_device_train_batch_size=16,  \n    per_device_eval_batch_size=16,   \n    weight_decay=0.02,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir='./logs',            \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}